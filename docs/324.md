# 机器会做梦吗？就玻尔兹曼机器和大脑采访特里·塞伊诺夫斯基

> 原文：<https://blog.paperspace.com/terry-sejnowski-boltzmann-machines/>

[Terry Sejnowski](https://www.salk.edu/scientist/terrence-sejnowski/) 是深度学习的先驱之一，他与 [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) 一起创造了 [Boltzmann machines](https://en.wikipedia.org/wiki/Boltzmann_machine) :一个与大脑中的学习有着惊人相似之处的深度学习网络。最近看了 Terry 的精彩著作[深度学习革命](https://www.amazon.com/Deep-Learning-Revolution-MIT-Press/dp/026203803X)后，和他进行了一次对话。我们谈到了深度学习和神经科学之间的融合，以及机器是否会做梦。

**特里·塞伊诺夫斯基:**

> 我被培养成一名理论物理学家，但却对大脑着迷。我们用与今天的计算机相比实在微不足道的计算机做模拟。当时机器学习还处于起步阶段。

> 杰夫和我进入了受大脑组织方式生物启发的神经网络。但是就连接性而言，这些要简单得多。它们只是简单的非线性函数。但是它们足够复杂，我们试图用它们来解决视觉中的复杂问题。

> 视觉似乎很简单，因为当我们早上睁开眼睛时，我们看到的是物体。似乎不需要任何努力，任何思考。那个机器是不透明的。我们没看懂。我们那时对复杂程度没有概念，也不知道大脑有多少计算能力。

> 在大脑中，大脑表面的大脑皮层代表了世界以及你所有的计划和行动。这是大脑中最高层次的处理。有一个突出的问题，那就是你如何在一个有这么多复杂层次的系统中学习。

> 我们的目标是尝试建立一个多层网络——一个输入层、一个输出层和中间的层——并让它学习。人们普遍认为，由于人工智能在 60 年代所做的早期工作，没有人会找到这样的学习算法，因为它在数学上太难了。

> 就在那时，Geoff 和我发明了玻尔兹曼机器，其架构灵感来自物理学。与当时考虑的所有其他架构不同的是，它是概率性的。

> 在当时可用的模型中，输入以确定的方式经历一系列阶段。但是我们没有自动得到相同的输出，我们认为如果每个单元都有一个概率，输出随着你给它的输入量而变化，也许我们可以取得进展。所以，输入越多，产生输出的概率就越高。如果投入低，产出的可能性就低。这带来了一定程度的可变性。

> 不仅如此，它创造了一个不同的网络阶层，这是生成性的。在传统的输入输出网络中，没有输入，就没有输出。基本上里面什么都没有。但是在玻尔兹曼机器中，即使没有输入，事物也在突突前进，因为总有一些可能性，每个单元都会有输出。这就是我们发现的在一个非常复杂的多层网络中学习的秘密，我们现在称之为深度学习。

> 我们给网络一个输入，然后跟踪网络中的活动模式。对于每个连接，我们跟踪输入和输出之间的相关性。然后为了能够学习——这都是我们做过的数学分析——你必须基本上摆脱输入，让它自由运行，在某种意义上让网络进入睡眠状态。但是，当然，它仍然在突突前进，你可以对每一对有连接的单元进行同样的测量。

> 你记录下这些相互关系。我们称之为睡眠阶段。学习算法非常简单。你从清醒学习阶段减去睡眠阶段相关性，这就是你如何改变重量的强度。要么上升，要么下降。我们表明，如果你这样做，并且你有足够大的数据集，那么你可以学习任意映射。我们训练它区分邮政编码上的手写数字。所以，有 10 个输出单元。你给它一些输入，这是一个手写的数字 2，然后在最顶端的单元，代表 2，将会在最高水平上比其他单元活跃。

> 这就是我们对数字的分类。但现在你可以做的是“箝位”，我们称之为“箝位”，或者固定 2 的输出，以便它是唯一有效的，其余的都关闭。因为这个网络有输入和输出，所以它向下渗透。这是一个高度循环的网络。它会开始创建看起来像 2 的输入，但它们会不断变化。顶部的环会来来去去，然后底部的环会来来去去，它们会四处游荡。所以这基本上是在做梦。这是一个关于 2-ness 的梦。这个网络创造了一个 2 的内在表现。

> 你阻止任何输入进来，这样网络可以表达一个在最高层次代表这个概念的输入。因此，信息不是从输入流向输出，而是从输出流向输入。这就是所谓的生殖网络。现在我们有了更强大的生成网络，生成对抗网络，这很神奇，因为你不仅可以生成 2，你还可以生成人脸的图片。你给它一堆像我们现在住的房间的例子，它会开始生成不存在的新房间，有不同种类的桌子、椅子和窗户，它们看起来都很真实，像照片一样逼真。这才是真正令人惊讶的地方，因为我们可以创建非常逼真的世界模型。

> 从某种意义上说，当我们睡着做梦时，大脑也是这样做的。我们看到的是根据我们的经验生成的模式。

> 杰夫和我完全相信我们已经弄清楚了大脑是如何工作的。为了在多层网络中学习，你不得不睡觉，这仅仅是巧合吗？人类每天晚上都要睡八个小时。我们为什么要睡觉？事实上，我帮助开拓的一个领域是试图真正理解当你睡着时你的大脑在想什么。

> 像我这样做计算模型的科学家在理解你白天的经历如何在晚上融入你的大脑方面取得了巨大的进步。这被称为记忆巩固，现在有大量的证据表明这就是正在发生的事情。

> 有一种叫做重放的东西发生在你大脑中对记忆很重要的部分，情节记忆，也就是发生在你身上的事情；事件独特的物品。在晚上，海马体会向大脑皮层回放这些经历，然后大脑皮层会将其整合到知识库中，这是你对世界的语义知识。玻尔兹曼机器的类比实际上是对睡眠过程中发生的事情的一个很好的洞察。但是现在，很明显，就神经元的数量和活动模式而言，睡眠期间真正发生的事情要复杂得多，我们已经对此进行了非常详细的研究。但是我们真的认为，从计算上来说，这就是正在发生的事情。

> 一方面，我们对大脑的知识，另一方面，我们现在有能力在大脑的图像中创建这些大规模的网络。不完全是，我们不是要复制大脑，而是从大脑中提取原理，尝试建立具有大脑某些功能的系统，比如视觉，比如语音识别，比如语言处理。

> 神经科学家正在观察深度学习发生了什么，受到启发，提出假设，并在大脑中进行测试。随着我们对大脑的了解越来越多，对大脑如何解决这些问题的了解越来越多，我们可以将其应用于深度学习。

> 考虑关注度。当我们环顾四周时，我们并不试图处理外面的一切。我们专注于一个特定的对象，专注于阅读一个句子。这意味着你必须集中注意力。事实证明，如果你关注这些深度学习网络，你会大大提高它们的表现。

> 如果你在做语言翻译，句子开头的一个单词可能和句子后面的一个单词有很强的关系。所以，当输入按顺序出现时，你希望能够抓住这些信息，注意它们。现在另一个词出现了，这两个词必须相互联系起来。

> 所以注意力是一种标记和表示这很重要的方式。牢记在心。然后在你把所有这些单词连接起来之后，它现在是一个有意义的表示。然后你开始用另一种语言输出单词。再次强调，尊重单词之间的关系，它们是如何排序的，它们的分句是什么样子的。而在德语中，你必须等到句子的末尾才能放动词。电视网必须明白这一点。它必须记住动词是什么，知道动词是什么，并且知道把它放在哪里。这都是我们认为理所当然的事情。这才是我们大脑真正擅长的。

> 因此，随着我们对大脑用于处理单词、语音、视觉等的机制了解得越来越多，这些将被整合并改善网络的性能。

> 现在，特别是自然语言处理，这已经达到了一个点，你可能从你的手机上知道，它真的很好。即使在嘈杂的环境中，语音识别也变得非常好。这是一个全新的时代。

> 我们的计算机视觉模型是基于摄像机的，而摄像机是基于帧的。所以当你拍摄视频时，它实际上是一系列包含图像的帧，然后你的大脑将它们组合成一个序列，这样你就可以看到运动并识别运动的东西。新一代的相机是基于你的视网膜的工作原理。你的视网膜实际上是大脑的一部分，它是你眼睛背面的一个小袋子，通过几层处理，它首先将图像转换为电信号，然后转换为尖峰信号。

> 流入大脑的脉冲编码了与颜色、运动和时间有关的信息。事物是如何随时间和相对强度变化的，例如，在一个边缘上，你有一个用尖峰编码的对比度变化。你有所有的信息。现在这串尖峰信号是异步的。与在 30 或 40 毫秒内收集信息的帧不同，您可以在任何时候发送尖峰信号。这意味着当世界上发生什么事情时，你可以在一毫秒或更短的时间内发出尖峰信号。尖峰信号的相对时间携带了很多关于事物走向的信息——比使用基于帧的相机要多得多。

> 如果你使用基于尖峰的表示法，我们称之为动态视觉传感器。它们的优点是功耗非常低，因为它们只发出这些尖峰信号。它们非常稀疏，因为如果没有东西在动，你实际上什么也没有得到。你必须有动作。而且非常轻便。例如，这对机器人来说是完美的事情，因为给机器人供电非常昂贵。如果你可以用尖峰信号而不是超级计算机或 GPU 来做视觉，这就是深度学习所用的东西，那么自治就更容易了。这就是我们要去的地方。

> 像你的手机和手表这样的边缘设备就是电脑，它们很快就会内置深度学习芯片。你必须有更好的电池，但最终如果你能用模拟超大规模集成电路取代数字电路，比如 DVS 相机，这将彻底改变你在手机上的计算量。

> 尖峰很有意思。大脑中的神经元发出这些脉冲，它们要么全是，要么没有，持续大约一毫秒。与数字电子设备相比，它们相对较慢。从这个意义上说，它们是模拟的。数字芯片有一个时钟，每个周期，每个晶体管都会更新，所以整个芯片必须同步。而这些模拟 VLSI 芯片是异步的。所以，每一个模型神经元都可以随时发出脉冲。

> 然后这些信号通过数字线路被传送到其他芯片。所以，这是一个混合芯片。它有模拟处理，真的很便宜，顺便说一下也不是很准确，不过没关系。事实证明，如果你用大量元素进行大量并行计算，然后整合这些信息，你会更好。但要在芯片之间进行通信，就像大脑那样，你必须将其转换成数字总线，并使用某种协议发送信息。

> 一旦我们意识到在多层网络中学习是可能的，那么几年内就发现了一堆其他的学习算法。最受欢迎的一种是误差反向传播，它要求你获取关于你做得有多好的信息，并将其与标记的输入进行比较，然后使用该误差反向更新权重。你总是在减少错误，而且你可以非常有效、非常迅速地做到这一点。因为它是如此的高效，它是现在大多数实际问题被越来越大的网络攻击的方式。

> 大脑的视觉皮层有 12 层。现在人们处理的网络有 200 层或更多。我们当时不知道的是，这是成功的关键，是这些学习算法可以很好地扩展。

> 人工智能中的一个典型算法能够解决一些小问题，在这些小问题中，你只有几个变量，你试图找到一个最优解。旅行推销员问题就是一个很好的例子。给定几个城市，如果你每个城市都去一次，最快的路线是什么？这被称为 NP 完全，这意味着随着城市数量的增加，问题变得更加困难。在某些时候，你的电脑有多快并不重要，问题会让它饱和。这就是在单处理器数字计算机中使用的许多算法的问题，这是冯诺依曼架构，内存与处理器分离。这两者之间有一个瓶颈。

> 现在，我们在 80 年代开创的这些神经网络的美妙之处在于它们是大规模并行的。这意味着他们使用简单的处理器，内存位于处理器上。它们在一起，所以你不必来回传递信息。在大脑中，我们有一千亿个并行工作的神经元。这意味着你可以实时进行更多的计算，而不必担心缓冲区或任何东西。随着你在网络中加入越来越多的神经元和越来越多的层，性能会越来越好。这意味着它的伸缩性非常好。

> 事实上，这绝对令人惊讶，如果您有并行硬件，也就是说，如果您同时模拟每个单元，并且同时通过连接权重传递信息，那么这就称为一级缩放。

> 这意味着所花的时间，与你拥有的单位数量无关。已经修好了。这就是大脑的工作方式。大脑工作正常。换句话说，随着大脑皮层在灵长类动物大脑中进化出越来越多的神经元，尤其是在人类大脑中，它仍然实时工作。它仍然在同样的时间内工作，以得出一个结论——仅仅识别一个物体就需要大约一百毫秒——没有比这更好的了。因此，大自然找到了一种扩大计算规模的方法。现在我们正在寻找答案。现在硬件已经成为机器学习的重要组成部分。直到最近，有了内存芯片，有了 CPU 芯片，可能还有一些数字信号处理芯片。

> 但是现在这些机器学习算法正在被植入硅片。谷歌已经有一个[张量处理单元](https://en.wikipedia.org/wiki/Tensor_processing_unit)，TPU，它做深度学习。但是还有很多其他的机器学习算法可以放入硅片中，这将极大地提高你可以做的计算量，因为这些就像现在的超级计算机。事实上，有一个来自[大脑](https://www.cerebras.net/)，它有 20 厘米宽，有 4 亿个处理单元。这已经达到了真实的规模。当然，这是千瓦，所以你必须有一个发电机，但它是按比例增加的。这是一种全新的芯片，人们刚刚开始欣赏它。

> 首先，它是异步的，这意味着你不需要芯片上的时钟。你可以让整件事过去。第二，你不需要 64 位精度。有八位也过得去。这意味着可以节省大量内存。然后在当地有高度的连通性。这意味着彼此靠近的处理器一直在交换大量信息。大脑也是这样工作的。现在加载所有的数据，就像它通过你的感官一样。它像管道一样流过。信息在流通，决策在制定。归根结底，这是一个极其复杂的动力系统。

> 我们现在面临着一个有趣的问题。我们真正想知道的是网络内部发生了什么。它学什么？现在最热门的事情是用神经科学在大脑上做的相同实验来探索人工神经网络。你怎么知道大脑里发生了什么？你把一个电极放在其中一个单元上，观察它有什么反应，当它有反应时。是在决定前开火还是决定后开火？这给了你信息是如何在网络中流动的提示。我们现在正在用这些人工网络做这件事。真的真的很刺激。

大脑可能介于玻尔兹曼机器和后道具网之间。这实际上导致了一个真正令人兴奋的新的研究领域，这是所有可能的并行计算系统，具有学习能力和接收大量数据并能够分类或预测的能力。我们只是触及表面。这是对这个空间全新的数学探索的开始。我写了一篇文章，最近被美国国家科学院学报接受。题目是深度学习的不合理有效性，因为深度学习是能够做不可解释的事情的。

这篇文章改编自播客 [Eye on AI](https://www.eye-on.ai/) 第 31 集的前半部分。在这里找到完整录制的剧集[，或者在下面收听。](https://www.eye-on.ai/podcast-031)

在采访的第一部分，Terry Sejnowski 向我们讲述了机器做梦，玻尔兹曼机器的诞生，大脑的内部运作，以及我们如何在人工智能中重现它们。

[//html5-player.libsyn.com/embed/episode/id/13028003/height/50/theme/legacy/thumbnail/no/direction/backward/](//html5-player.libsyn.com/embed/episode/id/13028003/height/50/theme/legacy/thumbnail/no/direction/backward/)