# 数据隐私的未来

> 原文：<https://blog.paperspace.com/the-future-of-data-privacy/>

正如许多人所知，数据访问是强大的监督学习模型在医疗保健等关键领域应用的限制之一。如果没有确保隐私的策略，这些数据将会被锁定。但是，新的策略正在出现，可以允许机器学习模型在没有真正看到这些数据的情况下根据这些数据进行训练。

英特尔人工智能产品部门的高级主管 Casimir Wierzynski 带我参观了最新的战略，这些战略有望解开解放人工智能所需的数据。Cas 还谈到了针对量子计算机的代码破解能力强化加密。

卡西米尔·维尔津斯基:

> 我目前在英特尔的职位是人工智能产品组的 CTO 办公室。我来自一个人文主义者的家庭，所以我现在在一个技术岗位上有点可笑。我妈妈是律师，我爸爸是记者。事实上，我最近从事的一些隐私保护工作，在某种程度上得益于我的新闻背景。

> 这些人工智能系统有如此多的前景，我们希望释放所有的潜力，并在我们周围的数据中找到所有这些好东西。人工智能系统从根本上是由数据塑造的。但是这些数据越来越隐私和敏感。你如何调和这两者？

> 关于隐私的整个领域，激励我的一件事是，隐私是一项人权。我爸爸来自波兰。他在 1945 年离开了波兰，先是去了欧洲，然后去了美国，但是在戒严期间，我在波兰还有表亲和叔叔等等。我记得人们必须向政府登记他们的打字机。

> 所以，我现在领导着一个团队，专门研究这个新兴领域，叫做“保护隐私的机器学习”。在保护隐私的机器学习保护伞下，有一套技术可以用来从数据中获得洞察力，而不必明确地查看数据的细节。

> 你想这么做有两个原因。一个是基本的安全和隐私，确保你没有数据泄露等等。但另一个原因是，现在您可以启用这些全新的应用程序。假设你有一组医院希望以某种方式共享他们的数据，以便在 MRI 扫描中建立一个更可靠的脑肿瘤检测器。统计学就是这样，你拥有的数据越多，你就越准确。但是很明显，共享病人数据是有问题的。

> 因此，有些情况下，人们想要提取数据，但出于很好的理由，他们不能。现在，使用这些隐私保护技术，你可以打开整个空间。不仅仅是医疗保健，金融服务业也是如此。如果你想检测邪恶的活动，欺诈，诸如此类的事情。银行显然想这么做，他们显然想集体行动来这么做。但是他们对数据有着非常真实的隐私担忧。

> 这是一个新兴领域，但已经有一些核心技术，它们正处于为现实世界做准备的不同阶段。因此，假设您想要共享数据；我刚刚谈到的例子是，人们希望集体操作数据，而不必明确地共享数据。有一些技术叫做联合学习或多方计算。所以那是一桶。另一个方面是你对加密数据的计算。因此，一方可以接收来自某人的加密数据，而无需解密，对这些数据进行某种运算。然后仍然给别人一个答案，即使他们从来没有看到潜在的数据。所以在我看来，这种技术是最神奇的，它叫做同态加密。围绕隐私还有另一套重要的技术。所以，如果你观察一个数据集，然后建立某种统计模型来学习这个数据集，从某种意义上来说，你不会想学得太多，你不会想记住那个数据集中的个别细节。你实际上想从中提取更大的不规则性。所以差别隐私是一种帮助你实现这一目标的技术。

> 我们可以依次讨论每一个，从联合学习开始。这实际上是一种直截了当的方式，尽管它的工作方式很酷。想象一下，你有几个人持有私有数据，我们称他们为联盟。这听起来有点像星际迷航。这是你的联盟，然后从一些机器学习模型的初始空白版本开始。联邦的每个成员都会得到那张白纸。然后他们中的每一个人，在他们的私人数据上，并且他们的私人数据从不离开他们的前提，弄清楚他们需要什么调整来使模型在他们的特定数据上更好地工作。他们每个人都想出了如何改变模型，使其更好地为自己服务。他们与某个中央协调者共享所有这些更新，然后这个协调者基本上将所有这些建议的更新加在一起。现在你得到了一个新的候选模型，这个新的候选模型再次被联盟的所有成员共享，然后你再次迭代。他们计算另一组调整。随着时间的推移，它们会来回移动。你最终得到的是一个能满足所有人数据的模型。这就好像您已经处理了池数据集，但您从未直接处理过任何数据。

> 您来回传输的不是整个模型，而是模型的参数，因为其中一些模型有数以亿计的参数，在每一轮中，这些建议的更新可能只涉及这些参数的一小部分，您只发送增量；测试变化。大约一年前，我们和宾夕法尼亚大学的一名放射科医生一起做了一项研究。有一个标准数据集叫做 BraTS，是一堆图像。任务是尝试从 MRI 图像中分割出脑瘤。这些图像是在不同的机构收集的。因此，如果我们假装已经以联合的方式训练了这个模型，我们从一个机构中取出所有的例子，并把它们放在一个桶中，并把每个机构分成桶，我们可以明确地测试这种想法，即联合训练是否会使你获得与只拥有所有数据接近的相同的性能。在这种情况下，它工作得非常好。围绕 is 还有一些有趣的研究思路。当联邦的不同成员拥有大相径庭的数据时，挑战就出现了。然后这个问题变得更加尖锐，你想多测试一点。我们对我的团队做了一些研究，提出了一些策略，在这些策略中，你可以适应性地改变更新过程的速度，以适应机构之间可能存在很大差异的事实。

> 假设您有一家医院，他们有足够的数据来满足自己的需求。即使在那里，也有一个很好的理由通过进行联合学习来扩展数据集，因为你可以确保你的模型实际上概括了底层的任务，而不是一些伪造的东西。

> 现在，同态加密是一种加密数据的方式，遵循希腊词源。同形意味着具有相同的形状。因此，当您将数据从未加密的世界移动到这个加密的空间时，数据相对于彼此仍然具有相同的形状。您仍然保留了数据之间的一些相对结构，但是您当然模糊了实际的数据是什么，因为它是加密的。特别是，在这个加密的世界里，一个简单的数字变成了一个非常高阶的多项式。你还记得从高中开始，多项式就是 2x + 3x <sup>2</sup> - 2x <sup>3</sup> 之类的东西，除了在这种情况下，x 的幂一直到，比如说，4096。在加密的世界里有这些非常大的多项式。

> 如果你在加密世界里把两个数字相加，然后把它们带回现实世界，你会得到你带来的两个东西的总和。因此，这是一种你可以在加密世界中操作的方式，以数学方式对应于现实世界中的操作，除了你从来没有真正看到底层数据。所以我们来举个具体的例子。医院进行扫描，他们使用自己的私钥加密扫描。现在是一堆多项式。他们把它发送到一些基于云的放射服务。放射科的服务完全是基于多项式的。他们不知道底层数据是什么。多项式回到医院，医院现在使用它的密钥，只有他们可以解锁这个东西，并获得他们正在寻找的诊断。

> 对于差分隐私，让我们来看一个例子，你的手机上有预测文本。你开始输入一个单词，然后你的手机预测下一个单词可能是什么。对于这种预测模型，你拿一堆短信数据，然后建立一个模型说，好吧，如果前一个词是“苹果”，那么下一个词可能是“馅饼”问题是，如果你的数据集中有人说，“我的信用卡号是，”然后他们说的下一件事就是他们的信用卡号，你不希望这种粒度级别出现在你的模型中。你不希望别人输入“我的信用卡号码是”，然后突然弹出你的号码。为了避免这种情况，我们使用差分隐私。有人称之为模糊数据或在训练数据中添加一点点噪声，以便迫使机器学习模型学习英语和基本语法的整体统计数据，但它不会有足够的统计能力来查看这些微小的细节，如个人信息。这是差异隐私的一个例子。

> 可能有二、三十多篇论文讨论了如何从某种模型中获得最大的效用，同时又能保护你所需要的隐私。在这个领域，隐私和实用性之间会有一点权衡。虽然我认为在实践中，人们发现在某种意义上有一个甜蜜点，一些模糊化可以使模型变得更好，因为机器学习的总体目标之一是泛化。概括的概念完全符合不要过度记忆数据集不同部分的想法，因为这些与你训练的任务没有密切关系。

> 有一种称为提取攻击的东西，实际上它与同态加密有很好的融合。机器学习模型实际上可以记忆或保存隐藏层中的数据，这些数据可以在训练后提取出来。你可以想到两种情况。一个是攻击者实际上可以访问模型，他们可以看到模型的所有参数以及它是如何构建的等等。这叫做白盒攻击。还有另一个版本，攻击者可以使用这个模型，他们可以向它发送东西并得到答案，但他们不能看到它是如何建立的。在这两种情况下，提取攻击都是可能的，但是如果您实际上能够访问模型的细节，那么提取攻击会容易得多。所以同态加密实际上可以成为保护模型细节的一种方式。我之前举的同态加密的例子是，我试图保护我提供给模型的数据的隐私。但是你也可以反过来，你可以加密模型，从而保护模型本身的机密性。

> 差分隐私解决了与同态加密略有不同的问题，因为差分隐私解决了将一组特定信息与特定个人联系起来的能力。同态加密和联合学习更多的是保密性。这些略有不同，但实际上是不同的问题，我们刚刚看了一个例子，在这个例子中，您可能希望两者都做，也就是说，您可能希望使用同态加密来防止有人训练数据来查看底层数据。但是，然后你会想使用差分隐私，以确保该模型没有学习训练集的细节。

> 我认为这有点像罗伯特·弗罗斯特的诗，“好篱笆造就好邻居。”如果我们可以做出很好的数学保证，我已经完成了这个机器学习操作，而你无法从数据中学到任何东西，你可以从个人和数据中学到的东西在数学上是有限的，我觉得这是一件正确的事情。就像你去超市看到麦片盒子上的成分表一样，这给了我信心，现在我可以出去买任何一盒麦片，并知道它会有某些特性。我认为这是人工智能成长所需要的基础工作。

> 我的设想，在某种程度上也是我的希望是，记得在网络早期，你在 Amazon.com 前面输入 HTTP，然后填写你的信用卡号。过了一会儿，人们说，‘嘿，你知道吗，好像我们不应该到处发信用卡号码。’然后对于某些非常敏感的事情，他们开发了这个叫做 HTTPS 的东西，这是一个安全的网络界面。然后人们逐渐被训练去寻找浏览器上的锁，这表明它是 HTTPS 的。但只有几页会被这样保护。过了一会儿，人们说，‘嘿，如果你能保护这一页，为什么不保护所有的页面呢？’？'

> 所以，现在几乎所有的东西都是 HTTPS 的。我觉得，对于机器学习来说，人们将对原始数据进行操作的想法会显得古怪、怪异和有点不雅。

> 为了达到 HTTPS 的目的，你需要一些东西。你需要使所有这些技术更加可用，这样做数据科学的人就不必担心在一些奇怪的空间里多项式有多大。您还需要某种程度的互操作性和围绕底层技术的行业共识。对于同态加密等一些事情来说，这已经开始发生了。我的团队成员正在与其他行业合作伙伴一起与正确的标准机构合作，以启动这一进程，使同态加密的各个方面标准化。围绕联合学习，其他标准团体已经做出了努力。所以我认为这将走到一起。

> 我们还没有谈到的一件事是，其中一些技术需要额外的计算。它们是计算密集型的。在 21 世纪初加密变得更加普遍的时候，英特尔曾经遇到过这种情况。有一种加密标准叫做 AES。我们在英特尔处理器产品线中添加了一条新指令，以加速加密方案的这一特定部分。因此，我们以前已经这样做了，我觉得对于其中一些协议，我们可能会再次这样做。我们需要提供硬件支持来加速这些非常特殊的计算。

> 可信执行环境是您可以在计算机上使用一定数量的内存来保持加密的方式。每当处理器需要访问内存时，它只会以加密的形式访问内存。然后，一旦它到达处理器的内部密室，只有到那时，我们才能解密它，进行某种操作，然后非常迅速地重新加密它。有一种方法可以加快这一过程，并使其更加有效。这绝对是我们可以使用的工具箱的一部分。

> 实际上，您可以两者都用，这是一种腰带和吊带的方法，您可以在其中一个可信区域内进行加密。假设两个不同的当事人都有他们想要保护的知识产权。一方拥有一个模型，而另一方拥有需要在该模型中操作的敏感数据。比方说，您可以使用同态加密来保护患者扫描，然后您可以使用 enclave 来加密模型。所以，现在你是在同时保护两个不同的党派。

> 密码社区正在密切关注量子计算机以及如何处理它。实际上，在 NIST 有一个过程，以前被称为标准局，他们正在研究什么是推荐的新加密系统，人们应该使用它来使它们成为所谓的后量子，或某种量子抵抗。

> 同态加密是一种称为基于格的加密方案的密码学家族，其中一些是后量子的。英特尔的一些人正在积极地向 NIST 建议下一个应该采用的标准是什么。其中一些是基于晶格的。

> 这些技术存在于一个受经济规律支配的世界，因此联合学习可能是一种方式，拥有私人数据仓库的人可以在不与任何人明确共享这些数据的情况下将这些数据货币化。这是一个非常有趣的可能性，因为它创造了市场，我以前是一名交易员，所以我喜欢围绕资源分配的市场机制。我认为这对这个领域来说是一个了不起的发展。

> 我的想法主要是围绕我们看到的直接的客户问题。所以我们还没有达到个人的水平，他们是否应该出售他们的个人数据。在医疗保健领域，已经有了数据共享协议，制药公司希望从医院获得数据，他们召集了大批律师，开出了一张大额支票。这是一个相当复杂的过程。促进这种企业对企业类型的互动将是一个非常好的起点。我知道杰伦·拉尼尔也在《纽约时报》上谈论个人出售他们的数据。这实际上是一个相当复杂的话题。有很多商业和社会上重要的 B2B 案例，我们希望尽快处理。

这篇文章改编自播客 [Eye on AI](https://www.eye-on.ai/) 第 32 集的后半部分。在这里找到完整录制的剧集[，或者在下面收听。](https://www.eye-on.ai/podcast-031)

[//html5-player.libsyn.com/embed/episode/id/13211051/height/50/theme/legacy/thumbnail/no/direction/backward/](//html5-player.libsyn.com/embed/episode/id/13211051/height/50/theme/legacy/thumbnail/no/direction/backward/)