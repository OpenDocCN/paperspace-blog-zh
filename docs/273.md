# 贝叶斯决策理论解释

> 原文:[https://blog.paperspace.com/bayesian-decision-theory/](https://blog.paperspace.com/bayesian-decision-theory/)

贝叶斯决策理论是模式分类的统计方法。它利用概率进行分类，并测量将输入分配给给定类别的风险(即成本)。

在这篇文章中，我们首先来看看先验概率，以及它为什么不是一种有效的预测方式。贝叶斯决策理论通过使用先验概率、似然概率和证据来计算后验概率，从而做出更好的预测。我们将详细讨论所有这些概念。最后，我们将把贝叶斯决策理论中的这些概念映射到它们在机器学习中的上下文中。

这篇文章的大纲如下:

*   先验概率
*   似然概率
*   先验和似然概率
*   贝叶斯决策理论
*   所有先验概率的总和必须为 1
*   所有后验概率之和必须为 1
*   证据
*   机器学习和贝叶斯决策理论
*   结论

完成这篇文章后，请继续关注第 2 部分，我们将把贝叶斯决策理论应用于二分类和多分类问题。为了评估分类器的性能，讨论了进行预测的损失和风险。如果分类器做出弱预测，则使用名为“拒绝”的新类别来接受具有高不确定性的样本。[第 2 部分](https://blog.paperspace.com/bayesian-decision-theory-machine-learning/)讨论何时以及为何将样品分配到剔除类别。

让我们开始吧。

## **先验概率**

讨论概率，要从如何计算一个动作发生的概率开始。概率是根据结果(即事件)的过去发生率计算的。这就是所谓的**先验概率**(“先验”的意思是“之前”)。换句话说，先验概率指的是过去的概率。

假设有人问未来两队比赛谁会赢。让$A$和$B$分别表示第一或第二队获胜。

在过去的 10 场杯赛中，$A$出现了 4 次，$B$出现了其余 6 次。那么，$A$在下一场比赛中出现的概率是多少？根据经验(即过去发生的事件)，第一队($A$)在下一场比赛中获胜的先验概率为:

$$ P(A)=\frac{4}{10}=0.4 $$

但是过去的事件不一定总是成立的，因为情况或背景可能会改变。例如，A$队可能只赢了 4 场比赛，因为有一些受伤的球员。当下一场比赛到来时，所有这些受伤的球员都将康复。根据目前的情况，第一队赢得下一场比赛的概率可能高于仅根据过去事件计算的概率。

先验概率测量下一个动作的概率，而不考虑当前观察(即当前情况)。这就像仅仅根据过去的医生出诊来预测一个病人患有某种疾病。

换句话说，因为先验概率仅仅是基于过去的事件(没有现在的信息)计算的，这可能降低预测值的质量。过去对两个结果$A$和$B$的预测可能在满足某些条件时发生，但是在当前时刻，这些条件可能不成立。

这个问题是用似然法解决的。

## **似然概率**

可能性有助于回答这个问题:给定一些条件，某个结果发生的概率是多少？它表示如下:

$$ P(X|C_i) $$

其中$X$表示条件，$C_i$表示结果。因为可能有多种结果，所以变量$C$被赋予下标$i$。

可能性如下所示:

> 在一组条件$X$下，结果为$C_i$的概率是多少？

根据我们预测获胜队的例子，结果$A$发生的概率不仅取决于过去的事件，还取决于当前的条件。可能性将结果的发生与做出预测时的当前条件联系起来。

假设情况发生变化，第一队没有受伤球员，而第二队有许多受伤球员。因此，$A$比$B$更有可能出现。不考虑当前情况，仅使用先验信息，结果将是$B$，这在当前情况下是不准确的。

对于诊断患者的例子，这可能是可以理解的更好的预测，因为诊断将考虑他们当前的症状而不是他们先前的状况。

仅使用可能性的缺点是它忽略了经验(先验概率)，而经验*在许多情况下*是有用的。因此，更好的预测方法是将两者结合起来。

## **先验概率和似然概率**

仅使用先验概率，根据过去的经验进行预测。仅使用可能性，预测仅取决于当前情况。当单独使用这两个概率中的任何一个时，结果都不够准确。在预测下一个结果时，最好同时使用经验和当前情况。

新的概率计算如下:

$$ {P(C_i)}{P(X|C_i)} $$

对于诊断患者的例子，结果将基于他们的病史以及他们当前的症状来选择。

同时使用先验概率和似然概率是理解贝叶斯决策理论的重要一步。

## **贝叶斯决策理论**

贝叶斯决策理论(即贝叶斯决策规则)不仅基于先前的观察，而且还通过考虑当前的情况来预测结果。**规则描述了根据观察结果采取的最合理的行动**。

贝叶斯(Bayes)决策理论的公式如下所示:

$ $ P(C _ I | X)= \ frac { P(C _ I)P(X | C _ I)} { P(X)} $ $

该理论的要素是:

*   $P(Ci)$:先验概率。这说明了类$C_i$独立于任何条件出现的次数(即，不考虑输入$X$)。
*   $P(X|Ci)$:可能性。在某些条件$X$下，这是结果$C_i$出现的次数。
*   $P(X)$:证据。条件$X$出现的次数。
*   $P(Ci|X)$:后验。给定某些条件$X$时，结果$Ci$发生的概率。

贝叶斯决策理论给出了平衡的预测，因为它考虑了以下因素:

1.  $P(X)$:条件$X$出现了多少次？
2.  $P(C_i)$:结果$C_i$出现了多少次？
3.  $P(X|C_i)$:条件$X$和结果$C_i$一起出现了多少次？

如果不使用前面的任何因素，预测就会受阻。让我们解释排除任何这些因素的影响，并提到一个使用每个因素可能有所帮助的案例。

*   $P(C_i)$假设不使用先验概率$P(C_i)$的情况；那么我们就无法知道结果$C_i$是否经常出现。如果先验概率很高，那么结果$C_i$频繁出现，并且它是它可能再次出现的指示。
*   $P(X|C_i)$如果不使用似然概率$P(X|C_i)$则没有信息将当前输入$X$与结果$C_i$相关联。例如，结果$C_i$可能经常出现，但是对于当前输入$X$很少出现。
*   $P(X)$如果排除证据概率$P(X)$则没有反映$X$出现频率的信息。假设结果$C_i$和输入$X$都频繁出现，那么当输入为$X$时，结果很可能是$C_i$。

当有关于$C_i$单独出现、$X$单独出现以及$C_i$和$X$一起出现的频率的信息时，可以做出更好的预测。

关于理论/规则，有一些事情需要注意:

1.  所有先验概率的总和必须为 1。
2.  所有后验概率之和必须为 1。
3.  证据是所有结果的先验概率和似然概率的乘积之和。

接下来的三个部分将逐一讨论这些要点。

### **所有先验概率之和必须为 1**

假设有两种可能的结果，那么必须满足以下条件:

$ $ P(C1)+P(C2)= 1 $ $

原因是对于一个给定的输入，其输出必须是这两者之一。没有未发现的结果。

如果有$K$个结果，则必须满足以下条件:

$ $ P(C1)+P(C2)+P(C3)+...+P(C_K)=1 $$

下面是使用求和运算符的写法，其中$i$是结果指数，$K$是结果总数:

$$ \sum_{i=1}^{K}P(C_i)=1 $$

请注意，以下条件必须适用于所有先验概率:

$$ P(C_i)>=0，\space \forall i $$

### **所有后验概率之和必须为 1**

类似于先验概率，根据下面的等式，所有后验概率的总和必须是 1。

$ $ P(C1 | X)+P(C2 | X)= 1 $ $

如果结果的总数是$K$，下面是使用求和运算符得出的总和:

$ $ P(C1 | X)+P(C2 | X)+P(C3 | X)+...+P(C_K|X)=1 $$

以下是如何使用求和运算符对$K$结果的所有后验概率求和:

$$ \sum_{i=1}^{K}P(C_i|X)=1 $$

### **证据**

以下是当只出现两种结果时，如何计算证据:

$ $ P(X)= P(X | C _ 1)P(C _ 1)+P(X | C _ 2)P(C _ 2)$

对于$K$ outcomes，以下是证据的计算方式:

$ $ P(X)= P(X | C _ 1)P(C _ 1)+P(X | C _ 2)P(C _ 2)+P(X | C _ 2)P(C _ 2)+...+P(X|C_K)P(C_K) $$

下面是使用求和运算符的写法:

$ $ p(x)=\sum_{i=1}^{k}p(x|c_i)p(c_i)$ $

根据最新的证据方程，贝叶斯决策理论(即后验)可以写成如下:

$ $ p(c_i|x)=\frac{p(c_i)p(x|c_i)}{\sum_{k=1}^{k}p(x|c_k)p(c_k)} $ $

## **机器学习&贝叶斯决策理论**

本节将机器学习中的概念与贝叶斯决策理论相匹配。

首先，**结局**这个词应该换成**类**。与其说结果是$C_i$，不如说类是$C_i$，这样对机器学习更友好。

以下是将贝叶斯决策理论中的因素与机器学习概念相关联的列表:

*   $X$是特征向量。
*   $P(X)$是特征向量$X$和用于训练模型的特征向量之间的相似度。
*   $C_i$是类别标签。
*   $P(C_i)$是模型将输入特征向量分类为类$C_i$的次数。该决定与特征向量$X$无关。
*   $P(X|C_i)$是之前的机器学习模型将类似于$X$的特征向量分类为类$C_i$的经验。这将类$C_i$与当前输入$X$联系起来。

当下列条件适用时，特征向量$X$可能被分配给类$C_i$:

*   在接近当前输入向量$X$的特征向量上训练该模型。这增加了$P(X)$的价值。
*   该模型在属于类$C_i$的一些样本(即特征向量)上被训练。这增加了$P(C_i)$。
*   该模型被训练以将接近$X$的样本分类为属于类$C_i$的。这增加了$P(X|Ci)$的价值。

当训练分类模型时，它知道类别$C_i$出现的频率，并且该信息被表示为先验概率$P(C_i)$。如果没有先验概率$P(C_i)$的话，分类模型会丢失一些学习到的知识。

假设先验概率$P(C_i)$是唯一要使用的概率，分类模型基于过去的观察对输入$X$进行分类，甚至没有看到新的输入$X$。换句话说，甚至不需要将样本(特征向量)提供给模型，模型就可以做出决定并将其分配给一个类别。

训练数据帮助分类模型将每个输入$X$映射到其类别标签$C_i$上。这种学习到的信息被表示为似然概率$P(X|C_i)$。在没有似然概率$P(X|C_i)$的情况下，分类模型无法知道输入样本$X$是否与类$C_i$相关。

## **结论**

本文介绍了机器学习背景下的贝叶斯决策理论。它描述了理论的所有要素，从先验概率$P(C)$开始，似然概率$P(X|C)$开始，证据$P(X)$开始，最后是后验概率$p(C|X)$结束。

然后，我们讨论了这些概念如何构建贝叶斯决策理论，以及它们如何在机器学习的环境中工作。

在下一篇文章中，我们将讨论如何[将贝叶斯决策理论应用于二元和多类分类问题](https://blog.paperspace.com/bayesian-decision-theory-machine-learning/)，看看损失和风险是如何计算的，最后，涵盖“拒绝”类的概念。